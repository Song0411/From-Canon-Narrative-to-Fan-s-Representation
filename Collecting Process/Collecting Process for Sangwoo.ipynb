{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colltecing Data From Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PRAW library to interact with the Reddit API\n",
    "import praw  \n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in d:\\anaconda\\lib\\site-packages (2.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in d:\\anaconda\\lib\\site-packages (from emoji) (4.10.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the emoji library to handle emoji removal and processing\n",
    "!pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji package installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(\"emoji package installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='wNgKgy6sNqNZ3Tiq6pviOg',       # My client ID\n",
    "    client_secret='TCQNcODtqQ3nE2LiJq89cy91MXYpug',  # My client secret\n",
    "    user_agent='Xinyi Song'                   # My user agent\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post_and_comments(submission_url):\n",
    "    # Access the Reddit submission using the provided URL\n",
    "    submission = reddit.submission(url=submission_url)\n",
    "    \n",
    "    # Retrieve the content of the main post\n",
    "    post_content = remove_emoji(submission.title + \"\\n\\n\" + submission.selftext)\n",
    "    \n",
    "    # Extract all comments from the post\n",
    "    comments = []\n",
    "    submission.comments.replace_more(limit=None)  # Ensure all comments are fully retrieved\n",
    "    for comment in submission.comments.list():\n",
    "        comments.append(remove_emoji(comment.body))  # Remove emojis from each comment\n",
    "    \n",
    "    return post_content, comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation and saving data into Excel\n",
    "import pandas as pd  \n",
    "\n",
    "# Import random to perform random sampling of comments\n",
    "import random  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Cho Sang-woo Reddit discussion links\n",
    "sangwoo_links = [\n",
    "    'https://www.reddit.com/r/squidgame/comments/qanci1/thoughts_on_sangwoo_as_a_character/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/q3wvu4/sang_woo_is_disgustingly_calculative_why_does/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/wtpl6f/sang_woo_is_not_evil_at_all_in_the_game/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/q8t8w6/people_who_are_hating_on_sangwoo_is_forgetting/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/11husa8/i_liked_sangwoo/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/qfkzsq/the_strength_of_sangwoos_moral_character/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/qg8p5m/sangwoo_wasnt_evil_in_the_games_imo/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/rduska/thoughts_on_cho_sangwoo/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/q6s8lp/sang_woo/',\n",
    "    'https://www.reddit.com/r/squidgame/comments/q4ej1e/whether_you_hate_him_or_not_sang_woo_is_a_well/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 800 comments globally across all posts\n",
    "def sample_comments_globally(all_contents, max_comments=800):\n",
    "    all_comments = []\n",
    "\n",
    "    # Collect all comments across all posts\n",
    "    for content in all_contents:\n",
    "        all_comments.extend(content['comments'])\n",
    "\n",
    "    # Randomly sample 800 comments if more than 800 exist\n",
    "    if len(all_comments) > max_comments:\n",
    "        all_comments = random.sample(all_comments, max_comments)\n",
    "\n",
    "    return all_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data and save it to an Excel file\n",
    "def save_sangwoo_corpus(links, filename):\n",
    "    all_contents = []\n",
    "\n",
    "    # Iterate through each link to scrape post content and comments\n",
    "    for link in links:\n",
    "        post_content, comments = scrape_post_and_comments(link)\n",
    "        all_contents.append({'post': post_content, 'comments': comments})\n",
    "\n",
    "    # Sample 800 comments globally\n",
    "    sampled_comments = sample_comments_globally(all_contents)\n",
    "\n",
    "    # Flatten the data for easy export\n",
    "    flattened_data = []\n",
    "\n",
    "    # Add post content to the flattened data\n",
    "    for content in all_contents:\n",
    "        flattened_data.append({'type': 'post', 'text': content['post']})\n",
    "\n",
    "    # Add sampled comments to the flattened data\n",
    "    for comment in sampled_comments:\n",
    "        flattened_data.append({'type': 'comment', 'text': comment})\n",
    "\n",
    "    # Convert the flattened data to a DataFrame and save to an Excel file\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "    df.to_excel(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Cho Sang-woo's corpus with a maximum of 800 comments\n",
    "save_sangwoo_corpus(sangwoo_links, 'Cho_Sang-woo_Reddit_Corpus.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reddit Finished, Next AO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make HTTP requests for accessing story pages\n",
    "import requests  \n",
    "\n",
    "# To parse the HTML content of the story pages\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "# To organize and save the data into a CSV file\n",
    "import pandas as pd  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Metadata Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(story_id):\n",
    "    # Construct the URL for the specific story using its ID\n",
    "    url = f\"https://archiveofourown.org/works/{story_id}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title = soup.find('h2', class_='title').text.strip() if soup.find('h2', class_='title') else None\n",
    "        \n",
    "        # Extract the author\n",
    "        author = soup.find('a', rel='author').text.strip() if soup.find('a', rel='author') else None\n",
    "        \n",
    "        # Extract the number of kudos (likes)\n",
    "        kudos = int(soup.find('dd', class_='kudos').text.strip().replace(',', '')) if soup.find('dd', class_='kudos') else 0\n",
    "        \n",
    "        # Extract the number of hits (views)\n",
    "        hits = int(soup.find('dd', class_='hits').text.strip().replace(',', '')) if soup.find('dd', class_='hits') else 0\n",
    "        \n",
    "        # Extract the number of bookmarks\n",
    "        bookmarks = int(soup.find('dd', class_='bookmarks').text.strip().replace(',', '')) if soup.find('dd', class_='bookmarks') else 0\n",
    "        \n",
    "        # Extract the rating\n",
    "        rating = soup.find('dd', class_='rating').text.strip() if soup.find('dd', class_='rating') else None\n",
    "        \n",
    "        # Extract the warnings\n",
    "        warnings = soup.find('dd', class_='warning').text.strip() if soup.find('dd', class_='warning') else None\n",
    "        \n",
    "        # Extract the category\n",
    "        category = soup.find('dd', class_='category').text.strip() if soup.find('dd', class_='category') else None\n",
    "        \n",
    "        # Extract relationships\n",
    "        relationships = [tag.text.strip() for tag in soup.find('dd', class_='relationship').find_all('a')] if soup.find('dd', class_='relationship') else []\n",
    "        \n",
    "        # Extract characters\n",
    "        characters = [tag.text.strip() for tag in soup.find('dd', class_='character').find_all('a')] if soup.find('dd', class_='character') else []\n",
    "        \n",
    "        # Extract additional tags\n",
    "        additional_tags = [tag.text.strip() for tag in soup.find('dd', class_='freeform').find_all('a')] if soup.find('dd', class_='freeform') else []\n",
    "        \n",
    "        # Return the extracted metadata\n",
    "        return {\n",
    "            'story_id': story_id,\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'kudos': kudos,\n",
    "            'hits': hits,\n",
    "            'bookmarks': bookmarks,\n",
    "            'rating': rating,\n",
    "            'warnings': warnings,\n",
    "            'category': category,\n",
    "            'relationships': relationships,\n",
    "            'characters': characters,\n",
    "            'additional_tags': additional_tags\n",
    "        }\n",
    "    else:\n",
    "        # Return None if the request fails\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Multiple Story IDs\n",
    "\n",
    "def main_metadata(story_ids, character_name):\n",
    "    data = []\n",
    "    # Loop through each story ID to collect its metadata\n",
    "    for story_id in story_ids:\n",
    "        story_data = get_metadata(story_id)\n",
    "        if story_data:\n",
    "            data.append(story_data)\n",
    "    \n",
    "    # Save the collected metadata to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    file_name = f'{character_name}_AO3_metadata.csv'\n",
    "    df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Metadata saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Sang-woo Story IDs\n",
    "cho_sangwoo_ids = [\n",
    "    34127974, 34079449, 34448059, 34041061, 34133302, 34203805,\n",
    "    34387306, 34297090, 34406347, 34139971, 34298500, 34130629,\n",
    "    34265242, 34444378, 34316797, 34089418, 34339738, 34269919,\n",
    "    34199386, 34432015, 34426993, 34065412, 34287916, 34410592,\n",
    "    34379929, 34351207, 34280716, 34080937, 34121575, 34294075\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to Cho_Sang-woo_AO3_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect and save metadata for Cho Sang-woo stories\n",
    "main_metadata(cho_sangwoo_ids, 'Cho_Sang-woo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full text from AO3\n",
    "def get_full_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title = soup.find('h2', class_='title').text.strip() if soup.find('h2', class_='title') else None\n",
    "        \n",
    "        # Extract the author\n",
    "        author = soup.find('a', rel='author').text.strip() if soup.find('a', rel='author') else None\n",
    "        \n",
    "        # Extract the full story text\n",
    "        full_text = soup.find('div', class_='userstuff').get_text(strip=True) if soup.find('div', class_='userstuff') else None\n",
    "        \n",
    "        # Return the data as a dictionary\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'full_text': full_text\n",
    "        }\n",
    "    else:\n",
    "        # Return None if the request fails\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape full text for a list of story URLs\n",
    "def scrape_full_text(urls, character_name):\n",
    "    data = []\n",
    "    # Loop through each URL to collect full text data\n",
    "    for url in urls:\n",
    "        story_data = get_full_text(f'https://archiveofourown.org/works/{url}')\n",
    "        if story_data:\n",
    "            data.append(story_data)\n",
    "    \n",
    "    # Save the collected full text data to a CSV file\n",
    "    df = pd.DataFrame(data)\n",
    "    file_name = f'{character_name}_AO3_full_text.csv'\n",
    "    df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Full text saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cho Sang-woo URLs\n",
    "cho_sangwoo_urls = [\n",
    "    34127974, 34079449, 34448059, 34041061, 34133302, \n",
    "    34203805, 34387306, 34297090, 34406347, 34139971, \n",
    "    34298500, 34130629, 34265242, 34444378, 34316797, \n",
    "    34089418, 34339738, 34269919, 34199386, 34432015, \n",
    "    34426993, 34065412, 34287916, 34410592, 34379929, \n",
    "    34351207, 34280716, 34080937, 34121575, 34294075\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text saved to Cho_Sang-woo_AO3_full_text.csv\n"
     ]
    }
   ],
   "source": [
    "# Scraping full text for Cho Sang-woo's stories\n",
    "scrape_full_text(cho_sangwoo_urls, 'Cho_Sang-woo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
